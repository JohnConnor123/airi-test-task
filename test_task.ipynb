{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8cc9c83-056f-42bb-bcf2-183c2bea20fa",
   "metadata": {},
   "source": [
    "# Protein stability ∆∆G prediction\n",
    "Predicting protein stability changes due to mutations is a critical task in bioinformatics, with applications in drug design, protein engineering, and understanding disease mechanisms. In this task, you are provided with feature representations of protein pairs (wild type and mutant type) and are required to predict the stability change (∆∆G) resulting from the mutations. Only single substitution mutations are considered. Single substitution mutation is when a single amino acid in the protein is changed to another one.\n",
    "\n",
    "## Provided Data\n",
    "You will work with two datasets. \n",
    "- A subset of [PROSTATA](https://www.biorxiv.org/content/10.1101/2022.12.25.521875v1) dataset. Contains features calculated with [OpenFold](https://github.com/aqlaboratory/openfold) for 2375 mutations. This dataset will be used as training dataset. Target ∆∆G scores are provided.\n",
    "- A test dataset that does not contain any proteins homologous to the training set.  Contains features calculated with [OpenFold](https://github.com/aqlaboratory/openfold) for 907 mutations. This dataset will be used as test dataset. **Target ∆∆G scores are not provided.** In this notebook, ∆∆G scores are actually known to show how the metrics can be calculated.\n",
    "\n",
    "## Baseline model\n",
    "In this notebook, we provide the code that preprocesses data, creates an `MLP` model and trains on mutations from PROSTATA. It also calculates the metrics on test dataset. Note that target scores will not be available.\n",
    "\n",
    "## Submission Format\n",
    "Your submission should include:\n",
    "\n",
    "- Reproducible code that trains the final model.\n",
    "- Predictions CSV: A CSV file containing your predicted ∆∆G values for the test dataset.\n",
    "- Technical Report: A detailed report explaining your approach, including:\n",
    "    + Model selection and training process.\n",
    "    + Evaluation results and analysis.\n",
    "    + Any challenges faced and how they were addressed.\n",
    "    + Possible improvements and future work.\n",
    "\n",
    "## Requirements\n",
    "- `python>=3.9`\n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `torch`\n",
    "- `torchvision`\n",
    "- `scipy`\n",
    "- `sklearn`\n",
    "\n",
    "## Conclusion\n",
    "In this task, you are expected to leverage your machine learning skills to predict protein stability changes. We encourage you to explore different models, feature engineering techniques, and hyperparameter tuning to improve your predictions. Your technical report should reflect your thought process, experimentation, and insights gained during the task.\n",
    "\n",
    "**Good luck!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfea1aae-5e08-4129-825d-2738779a590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da62b6b7-70e0-4699-95b6-617f46acedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a9dc885-f44e-4108-a265-b6d81742be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from protein_task import ProteinTask, get_protein_task, get_feature_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a37c813-d457-430a-81a0-4208c5f7f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9906b041-2b2a-4bb4-9252-1eeddd169866",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b1aeb8-d4ba-4e81-91b0-ac7259e68f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/prostata_filtered.csv\")\n",
    "target = torch.tensor(df_train[\"ddg\"], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c7c23-a610-4241-85d4-5507dda07e4a",
   "metadata": {},
   "source": [
    "Load protein features. Protein is represented as a ```ProteinTask``` class object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df21a46f-27af-4fbe-bc3c-ff5cc487075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tasks = \"data/prostata_test_task\"\n",
    "all_tasks = []\n",
    "\n",
    "for idx in range(len(df_train)):\n",
    "    task = get_protein_task(df_train, idx=idx, path=path_to_tasks)\n",
    "    all_tasks.append(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63d30d-ebb7-4482-8faa-4ff768319cc4",
   "metadata": {},
   "source": [
    "Load test protein features. **Note that DDG for test dataset is unavailable and only given here and below as an example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e3c2aaf-da36-426b-8767-5a5525e4d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ssym = pd.read_csv(\"data/ssym.csv\")\n",
    "df_test_s669 = pd.read_csv(\"data/s669.csv\")\n",
    "df_test = pd.concat((df_test_ssym, df_test_s669), axis=\"rows\", ignore_index=True)\n",
    "\n",
    "# test_target = torch.tensor(df_test[\"ddg\"], dtype=torch.float32) # test DDG not available\n",
    "test_target = torch.zeros(df_test.shape[0], dtype=torch.float32) # Note that this is FAKE target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f94fb4-1022-4e2b-82ab-aa0cb3f16082",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_tasks = []\n",
    "path_to_test_tasks = {\n",
    "    \"ssym\": \"data/ssym_test_task\",\n",
    "    \"s669\": \"data/s669_test_task\"\n",
    "}\n",
    "\n",
    "for idx in range(len(df_test)):\n",
    "    source = df_test.iloc[idx][\"source\"]\n",
    "    task = get_protein_task(df_test, idx=idx, path=path_to_test_tasks[source])\n",
    "    test_all_tasks.append(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a9edd-5add-480e-891c-3f5cabcc46b0",
   "metadata": {},
   "source": [
    "`ProteinTask` object has three fields:\n",
    "- `task` stores general information about the protein: path to the `pdb` file, list of mutations, and the numbering of residues in the protein (`obs_positions`). Mutations are stored as dictionaries : `{(<wild type amino acid>, <position of mutation>, <chain id>): <mutant type amino acid>}`. Please note that numbering of residues in proteins does not always start with `0` so the correct position of the mutation does not correspond to `<position of mutation>` in general.\n",
    "- `protein_of` contains features precomputed with OpenFold for both wild type and mutant type proteins as well as `pd.DataFrame` representations of proteins. The features are represented as dictionaries: `{\"<amino acid>_<chain_id>_<position>\": features_dict}`, where `features_dict` is itself a dictionary containing all OpenFold outputs for a specific residue:\n",
    "```\n",
    "'msa' tensor, shape=(256,)\n",
    "'pair' tensor, shape=(128,)\n",
    "'lddt_logits' tensor, shape=(50,)\n",
    "'distogram_logits' tensor, shape=(64,)\n",
    "'aligned_confidence_probs' tensor, shape=(64,)\n",
    "'predicted_aligned_error' tensor, shape=(1,)\n",
    "'plddt' tensor, shape=(1,)\n",
    "'single' tensor, shape=(384,)\n",
    "'tm_logits' tensor, shape=(64,)\n",
    "```\n",
    " Note that `pair`, `distogram_logits` and `aligned_confidence_probs` are calculated for each pair of residues in the protein, so the full tensors have the shape of `[num_residues x num_residues x embedding_dim]`. However, we are limited in terms of the size of the dataset, so only the diagonal elements are taken from full tensors. For example, `pair` representations for residue `idx` is calculated as the corresponding diagonal vector of the full pair representation tensor: `pair = pair_initial[idx, idx, :]`. Refer to [AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2) paper and [OpenFold](https://github.com/aqlaboratory/openfold) for more information.\n",
    "- `protein_job` contains `pd.DataFrame` representations for both wild type and mutant type proteins as well as a mapping from numbering of residues in the protein to their corresponding index in the features tensor. The mapping `obs_positions` is a dictionary `{<amino acid>_<chain_id>_<position>: <feature index>}`.\n",
    "\n",
    "Next, we demonstrate how to use the `obs_positions` mapping to get features of mutated amino acid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e72e8f-f468-40fa-b875-b05867bfbaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([358])\n"
     ]
    }
   ],
   "source": [
    "example_task = all_tasks[1234]\n",
    "mutation = example_task.task['mutants']\n",
    "\n",
    "# there is only one mutation for proteins in PROSTATA so take the first element of the dictionary\n",
    "mutation_key, _ = next(iter(mutation.items()))\n",
    "res_name, position, chain_id = mutation_key\n",
    "\n",
    "# translate mutation key to feature index: \"<amino acid>_<chain_id>_<position>\"\n",
    "residue_name = '_'.join((res_name, chain_id, str(position)))\n",
    "feature_index = example_task.protein_job['protein_wt']['obs_positions'][residue_name]\n",
    "\n",
    "# feature index of mutated aminoacid is the same; the name of the amino acid in the mapping is not changed\n",
    "assert feature_index == example_task.protein_job['protein_mt']['obs_positions'][residue_name]\n",
    "\n",
    "# get OpenFold features corresponding to the mutated amino acid of the wild type and mutant type protein\n",
    "feature_tensor = get_feature_tensor(example_task, feature_names=[\"pair\", \"lddt_logits\", \"plddt\"]) # feel free to experiment with different features :)\n",
    "features = torch.cat((feature_tensor['wt'][feature_index], feature_tensor['mt'][feature_index]), dim=0)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f86f2-9df0-4a94-a4ac-cc71ad37c48e",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "629d7a16-4e1c-49f2-ad6a-f9dde8dad7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "537a5757-b0ea-44ec-b8e1-c8b045e6f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for task in all_tasks:\n",
    "    mutation = task.task['mutants']\n",
    "    mutation_key, _ = next(iter(mutation.items()))\n",
    "    res_name, position, chain_id = mutation_key\n",
    "    residue_name = '_'.join((res_name, chain_id, str(position)))\n",
    "    feature_index = task.protein_job['protein_wt']['obs_positions'][residue_name]\n",
    "    feature_tensor = get_feature_tensor(task, feature_names=[\"pair\", \"lddt_logits\", \"plddt\"]) \n",
    "    features.append(torch.cat((feature_tensor['wt'][feature_index], feature_tensor['mt'][feature_index]), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac718e47-db9a-4a20-94fc-287fa6902be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = []\n",
    "for task in test_all_tasks:\n",
    "    mutation = task.task['mutants']\n",
    "    mutation_key, _ = next(iter(mutation.items()))\n",
    "    res_name, position, chain_id = mutation_key\n",
    "    residue_name = '_'.join((res_name, chain_id, str(position)))\n",
    "    feature_index = task.protein_job['protein_wt']['obs_positions'][residue_name]\n",
    "    feature_tensor = get_feature_tensor(task, feature_names=[\"pair\", \"lddt_logits\", \"plddt\"]) \n",
    "    features_test.append(torch.cat((feature_tensor['wt'][feature_index], feature_tensor['mt'][feature_index]), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6f54ef3-f9f8-4675-b765-eaae2c40bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.stack(features, dim=0), target[:, None])\n",
    "test_dataset = TensorDataset(torch.stack(features_test, dim=0), test_target[:, None])\n",
    "dataloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e43da23-4966-4836-98cc-68d128bab112",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bf929-40c9-4c1c-8e34-df81f4aee432",
   "metadata": {},
   "source": [
    "Create a model. We chose a simple MLP as a baseline for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7502bbd3-2d62-4a21-9eab-891449ee0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e03c3966-8f9b-4204-86e2-2bffcc1c5417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(MLP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        dim_hidden,\n",
    "        num_layers=3,\n",
    "        norm_layer=None,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        hidden_channels = [dim_hidden] * (num_layers - 1) + [1]\n",
    "        super(MLPHead, self).__init__(\n",
    "            in_channels,\n",
    "            hidden_channels,\n",
    "            inplace=False,\n",
    "            norm_layer=norm_layer,\n",
    "            dropout=dropout\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76c17795-a7d2-4847-b910-eb7bf345be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPHead(in_channels=features[0].size(0), dim_hidden=128, dropout=0.5, norm_layer=torch.nn.BatchNorm1d).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44d9c3-a45d-4a91-aeee-7b5633a91ce9",
   "metadata": {},
   "source": [
    "### Optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aa781ca-4a81-4425-9d8a-b4e8d7e65039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9d67857-252b-4e42-ac66-646b22a332e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab6c9d53-9e4c-4c8e-b41c-88a783d7f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c09b93-a9f7-4da4-a0ac-7f1e48753d44",
   "metadata": {},
   "source": [
    "### Train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39797884-7cab-42bf-a4ae-76ebbf50e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b29dfc-7247-4157-863b-0c5ae445dc57",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "794fc0cc-39a1-4461-86d9-8edb3ebadf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataloader):\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(DEVICE)\n",
    "        \n",
    "            # Make predictions for this batch\n",
    "            outputs.append(model(inputs))\n",
    "    outputs = torch.cat(outputs, dim=0).cpu()\n",
    "    return outputs  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cea33fe-2a94-4d63-99b6-75bfa9a62536",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "`compute_metrics` calculates various metrics. We consider three types of metrics. \n",
    "\n",
    "Regression metrics.\n",
    "- **R2**\n",
    "- **Spearman correlation coefficient**\n",
    "- **Pearson correlation coefficient**\n",
    "- **RMSE**\n",
    "\n",
    "Classification metrics. The mutation is considered stabilizing (label=+1) if the DDG is less than -0.5. Otherwise, the mutation is considered destabilizing (label=-1).\n",
    "- **AUC score**\n",
    "- **Accuracy**\n",
    "- **Matthews correlation coefficient**\n",
    "\n",
    "We consider how well the model performs on stabilizing mutations only:\n",
    "- **DetPr**. Precision of the model among 30 most stabilizing mutations\n",
    "- **StabSpearman**. Spearman correlation coefficient for stabilizing mutations only\n",
    "\n",
    "Additionally, we calculate how well the model ranks the mutations (**nDCG@30**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43ea7319-923e-484f-bbc6-3b6a167e6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53f7cb9e-db0c-4844-a374-e43c54f59b91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 4.463985466957093\n",
      "  batch 20 loss: 4.619276475906372\n",
      "  batch 30 loss: 4.499901747703552\n",
      "  batch 40 loss: 4.05017614364624\n",
      "  batch 50 loss: 4.218283939361572\n",
      "  batch 60 loss: 5.160892057418823\n",
      "  batch 70 loss: 5.052144575119018\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 5.422458958625794\n",
      "  batch 20 loss: 3.930216598510742\n",
      "  batch 30 loss: 3.30447404384613\n",
      "  batch 40 loss: 3.748665475845337\n",
      "  batch 50 loss: 4.383455348014832\n",
      "  batch 60 loss: 3.962450122833252\n",
      "  batch 70 loss: 4.123427772521973\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 3.9366216897964477\n",
      "  batch 20 loss: 4.178701710700989\n",
      "  batch 30 loss: 4.752433168888092\n",
      "  batch 40 loss: 3.115480124950409\n",
      "  batch 50 loss: 3.287083649635315\n",
      "  batch 60 loss: 3.6717379570007322\n",
      "  batch 70 loss: 3.888295555114746\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 3.414075231552124\n",
      "  batch 20 loss: 3.7770638942718504\n",
      "  batch 30 loss: 4.351292860507965\n",
      "  batch 40 loss: 4.138594436645508\n",
      "  batch 50 loss: 3.3688727617263794\n",
      "  batch 60 loss: 3.611896848678589\n",
      "  batch 70 loss: 3.7135064363479615\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 3.4074804306030275\n",
      "  batch 20 loss: 3.246730184555054\n",
      "  batch 30 loss: 3.4771592140197756\n",
      "  batch 40 loss: 2.4673285603523256\n",
      "  batch 50 loss: 3.757081115245819\n",
      "  batch 60 loss: 3.0763651490211488\n",
      "  batch 70 loss: 4.451698136329651\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 4.409188652038575\n",
      "  batch 20 loss: 2.9495905518531798\n",
      "  batch 30 loss: 3.441822075843811\n",
      "  batch 40 loss: 4.688310039043427\n",
      "  batch 50 loss: 3.1008886814117433\n",
      "  batch 60 loss: 3.6719499111175535\n",
      "  batch 70 loss: 3.7434125423431395\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 3.215246319770813\n",
      "  batch 20 loss: 2.73034610748291\n",
      "  batch 30 loss: 3.6337977170944216\n",
      "  batch 40 loss: 4.7243585109710695\n",
      "  batch 50 loss: 3.5076014518737795\n",
      "  batch 60 loss: 3.590310788154602\n",
      "  batch 70 loss: 3.646508717536926\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 2.6564462900161745\n",
      "  batch 20 loss: 3.827846360206604\n",
      "  batch 30 loss: 3.1223634243011475\n",
      "  batch 40 loss: 4.270453608036041\n",
      "  batch 50 loss: 3.7147950649261476\n",
      "  batch 60 loss: 4.198809218406677\n",
      "  batch 70 loss: 4.160436224937439\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 3.800616979598999\n",
      "  batch 20 loss: 3.907058835029602\n",
      "  batch 30 loss: 3.44209645986557\n",
      "  batch 40 loss: 3.571168231964111\n",
      "  batch 50 loss: 3.4434154272079467\n",
      "  batch 60 loss: 3.157101809978485\n",
      "  batch 70 loss: 4.085878789424896\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 3.849554169178009\n",
      "  batch 20 loss: 3.0408621311187742\n",
      "  batch 30 loss: 4.253176522254944\n",
      "  batch 40 loss: 3.0978217363357543\n",
      "  batch 50 loss: 3.8246247291564943\n",
      "  batch 60 loss: 4.181526637077331\n",
      "  batch 70 loss: 3.1118719816207885\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 2.844525229930878\n",
      "  batch 20 loss: 3.8514227867126465\n",
      "  batch 30 loss: 4.21526792049408\n",
      "  batch 40 loss: 3.889810597896576\n",
      "  batch 50 loss: 3.3931700468063353\n",
      "  batch 60 loss: 3.227205193042755\n",
      "  batch 70 loss: 3.233567476272583\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 3.628940224647522\n",
      "  batch 20 loss: 4.450608563423157\n",
      "  batch 30 loss: 3.5356720447540284\n",
      "  batch 40 loss: 3.3941987991333007\n",
      "  batch 50 loss: 4.147802925109863\n",
      "  batch 60 loss: 2.884671711921692\n",
      "  batch 70 loss: 3.234542214870453\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 3.836407470703125\n",
      "  batch 20 loss: 2.560681772232056\n",
      "  batch 30 loss: 4.074192190170288\n",
      "  batch 40 loss: 4.234949588775635\n",
      "  batch 50 loss: 3.1734087228775025\n",
      "  batch 60 loss: 3.1479978799819945\n",
      "  batch 70 loss: 3.638721537590027\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 3.177628779411316\n",
      "  batch 20 loss: 3.8637333035469057\n",
      "  batch 30 loss: 3.895996403694153\n",
      "  batch 40 loss: 2.830682802200317\n",
      "  batch 50 loss: 3.059523117542267\n",
      "  batch 60 loss: 2.444702136516571\n",
      "  batch 70 loss: 3.937858748435974\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 3.9199371933937073\n",
      "  batch 20 loss: 2.8989638328552245\n",
      "  batch 30 loss: 2.776100528240204\n",
      "  batch 40 loss: 3.4747175931930543\n",
      "  batch 50 loss: 3.538648319244385\n",
      "  batch 60 loss: 4.567115688323975\n",
      "  batch 70 loss: 4.181380426883697\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 3.185621166229248\n",
      "  batch 20 loss: 3.502410089969635\n",
      "  batch 30 loss: 3.1796462297439576\n",
      "  batch 40 loss: 3.566883420944214\n",
      "  batch 50 loss: 3.045042109489441\n",
      "  batch 60 loss: 4.02969262599945\n",
      "  batch 70 loss: 2.7675061225891113\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 3.6962212324142456\n",
      "  batch 20 loss: 3.0881776332855226\n",
      "  batch 30 loss: 3.7730353593826296\n",
      "  batch 40 loss: 3.4948137283325194\n",
      "  batch 50 loss: 3.1246285319328306\n",
      "  batch 60 loss: 3.3171335220336915\n",
      "  batch 70 loss: 3.0398533940315247\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 4.389257097244263\n",
      "  batch 20 loss: 2.697470408678055\n",
      "  batch 30 loss: 3.4119114518165587\n",
      "  batch 40 loss: 3.1730385184288026\n",
      "  batch 50 loss: 3.813046598434448\n",
      "  batch 60 loss: 3.3374476075172423\n",
      "  batch 70 loss: 2.9905619621276855\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 3.7653029799461364\n",
      "  batch 20 loss: 3.1236972212791443\n",
      "  batch 30 loss: 3.805876922607422\n",
      "  batch 40 loss: 2.661593866348267\n",
      "  batch 50 loss: 3.5480570197105408\n",
      "  batch 60 loss: 4.17439284324646\n",
      "  batch 70 loss: 3.3304126977920534\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 2.6251058340072633\n",
      "  batch 20 loss: 3.6464975595474245\n",
      "  batch 30 loss: 4.161215496063233\n",
      "  batch 40 loss: 3.1717229962348936\n",
      "  batch 50 loss: 2.9873291015625\n",
      "  batch 60 loss: 3.7250452876091003\n",
      "  batch 70 loss: 3.70184862613678\n",
      "EPOCH 21:\n",
      "  batch 10 loss: 3.8409742593765257\n",
      "  batch 20 loss: 3.986647826433182\n",
      "  batch 30 loss: 3.852978456020355\n",
      "  batch 40 loss: 3.725603795051575\n",
      "  batch 50 loss: 2.724222731590271\n",
      "  batch 60 loss: 2.916100251674652\n",
      "  batch 70 loss: 2.6295004963874815\n",
      "EPOCH 22:\n",
      "  batch 10 loss: 3.2014802217483522\n",
      "  batch 20 loss: 3.2597239017486572\n",
      "  batch 30 loss: 2.9620797276496886\n",
      "  batch 40 loss: 3.862345826625824\n",
      "  batch 50 loss: 3.4818145990371705\n",
      "  batch 60 loss: 2.832105040550232\n",
      "  batch 70 loss: 3.1447388768196105\n",
      "EPOCH 23:\n",
      "  batch 10 loss: 2.609643030166626\n",
      "  batch 20 loss: 2.8286417722702026\n",
      "  batch 30 loss: 2.853269600868225\n",
      "  batch 40 loss: 3.041397738456726\n",
      "  batch 50 loss: 4.599887871742249\n",
      "  batch 60 loss: 3.370189106464386\n",
      "  batch 70 loss: 2.929969835281372\n",
      "EPOCH 24:\n",
      "  batch 10 loss: 4.886402821540832\n",
      "  batch 20 loss: 2.694924199581146\n",
      "  batch 30 loss: 3.891831135749817\n",
      "  batch 40 loss: 2.549309992790222\n",
      "  batch 50 loss: 2.9961974024772644\n",
      "  batch 60 loss: 3.8579976320266725\n",
      "  batch 70 loss: 2.898625123500824\n",
      "EPOCH 25:\n",
      "  batch 10 loss: 3.9239981174468994\n",
      "  batch 20 loss: 2.8435242533683778\n",
      "  batch 30 loss: 3.52146555185318\n",
      "  batch 40 loss: 4.086942589282989\n",
      "  batch 50 loss: 3.252124524116516\n",
      "  batch 60 loss: 3.213480997085571\n",
      "  batch 70 loss: 2.7146180391311647\n",
      "EPOCH 26:\n",
      "  batch 10 loss: 2.832833099365234\n",
      "  batch 20 loss: 3.2054617643356322\n",
      "  batch 30 loss: 4.033487248420715\n",
      "  batch 40 loss: 3.5215681076049803\n",
      "  batch 50 loss: 2.670540511608124\n",
      "  batch 60 loss: 3.2003587007522585\n",
      "  batch 70 loss: 3.3366729974746705\n",
      "EPOCH 27:\n",
      "  batch 10 loss: 3.2662140846252443\n",
      "  batch 20 loss: 3.213392585515976\n",
      "  batch 30 loss: 3.320556330680847\n",
      "  batch 40 loss: 2.778002905845642\n",
      "  batch 50 loss: 4.765038394927979\n",
      "  batch 60 loss: 3.343714380264282\n",
      "  batch 70 loss: 2.938368761539459\n",
      "EPOCH 28:\n",
      "  batch 10 loss: 3.9986518740653993\n",
      "  batch 20 loss: 3.6977318286895753\n",
      "  batch 30 loss: 2.994576334953308\n",
      "  batch 40 loss: 3.4268595218658446\n",
      "  batch 50 loss: 2.8424306869506837\n",
      "  batch 60 loss: 3.2810789465904238\n",
      "  batch 70 loss: 2.7289796352386473\n",
      "EPOCH 29:\n",
      "  batch 10 loss: 2.369808828830719\n",
      "  batch 20 loss: 3.5797620177268983\n",
      "  batch 30 loss: 3.6595261096954346\n",
      "  batch 40 loss: 2.9385798454284666\n",
      "  batch 50 loss: 3.1826929450035095\n",
      "  batch 60 loss: 2.9198187351226808\n",
      "  batch 70 loss: 4.0437329649925235\n",
      "EPOCH 30:\n",
      "  batch 10 loss: 2.3597846746444704\n",
      "  batch 20 loss: 3.8465505123138426\n",
      "  batch 30 loss: 3.1847431302070617\n",
      "  batch 40 loss: 4.248413491249084\n",
      "  batch 50 loss: 2.2456936120986937\n",
      "  batch 60 loss: 3.2028146982192993\n",
      "  batch 70 loss: 2.8313553094863892\n",
      "EPOCH 31:\n",
      "  batch 10 loss: 3.4927676916122437\n",
      "  batch 20 loss: 3.339604651927948\n",
      "  batch 30 loss: 3.9736844062805177\n",
      "  batch 40 loss: 2.4462196588516236\n",
      "  batch 50 loss: 2.839940440654755\n",
      "  batch 60 loss: 2.505854535102844\n",
      "  batch 70 loss: 4.000183320045471\n",
      "EPOCH 32:\n",
      "  batch 10 loss: 3.1847575664520265\n",
      "  batch 20 loss: 2.670196306705475\n",
      "  batch 30 loss: 2.7551769375801087\n",
      "  batch 40 loss: 2.3139249205589296\n",
      "  batch 50 loss: 3.3496174216270447\n",
      "  batch 60 loss: 5.855137586593628\n",
      "  batch 70 loss: 3.1462159395217895\n",
      "EPOCH 33:\n",
      "  batch 10 loss: 3.5777377843856812\n",
      "  batch 20 loss: 2.9057900667190553\n",
      "  batch 30 loss: 3.1583499670028687\n",
      "  batch 40 loss: 3.1211203932762146\n",
      "  batch 50 loss: 3.4902878522872927\n",
      "  batch 60 loss: 3.1439733266830445\n",
      "  batch 70 loss: 3.4871551394462585\n",
      "EPOCH 34:\n",
      "  batch 10 loss: 3.855566620826721\n",
      "  batch 20 loss: 2.7472617983818055\n",
      "  batch 30 loss: 2.758903706073761\n",
      "  batch 40 loss: 3.441911315917969\n",
      "  batch 50 loss: 2.8092753052711488\n",
      "  batch 60 loss: 3.5179009675979613\n",
      "  batch 70 loss: 4.135835576057434\n",
      "EPOCH 35:\n",
      "  batch 10 loss: 3.1325477719306947\n",
      "  batch 20 loss: 3.5227200031280517\n",
      "  batch 30 loss: 2.496959555149078\n",
      "  batch 40 loss: 4.079634070396423\n",
      "  batch 50 loss: 2.928657066822052\n",
      "  batch 60 loss: 3.2345886826515198\n",
      "  batch 70 loss: 3.0693362593650817\n",
      "EPOCH 36:\n",
      "  batch 10 loss: 2.3944671154022217\n",
      "  batch 20 loss: 3.8269554376602173\n",
      "  batch 30 loss: 2.7084800958633424\n",
      "  batch 40 loss: 3.122708034515381\n",
      "  batch 50 loss: 2.7894106268882752\n",
      "  batch 60 loss: 2.920197939872742\n",
      "  batch 70 loss: 3.714509463310242\n",
      "EPOCH 37:\n",
      "  batch 10 loss: 2.381222832202911\n",
      "  batch 20 loss: 3.5714598178863524\n",
      "  batch 30 loss: 3.8094744205474855\n",
      "  batch 40 loss: 2.957316291332245\n",
      "  batch 50 loss: 2.7315916538238527\n",
      "  batch 60 loss: 3.081898903846741\n",
      "  batch 70 loss: 3.300808036327362\n",
      "EPOCH 38:\n",
      "  batch 10 loss: 3.6205566763877868\n",
      "  batch 20 loss: 2.813931465148926\n",
      "  batch 30 loss: 3.8228514552116395\n",
      "  batch 40 loss: 2.839130866527557\n",
      "  batch 50 loss: 3.4796313524246214\n",
      "  batch 60 loss: 2.5607653141021727\n",
      "  batch 70 loss: 2.7674795031547545\n",
      "EPOCH 39:\n",
      "  batch 10 loss: 3.8953327775001525\n",
      "  batch 20 loss: 2.8402331352233885\n",
      "  batch 30 loss: 3.1416593551635743\n",
      "  batch 40 loss: 3.747260868549347\n",
      "  batch 50 loss: 2.665040922164917\n",
      "  batch 60 loss: 3.6871660470962526\n",
      "  batch 70 loss: 3.2598382472991942\n",
      "EPOCH 40:\n",
      "  batch 10 loss: 3.2775654435157775\n",
      "  batch 20 loss: 3.3364279508590697\n",
      "  batch 30 loss: 3.550348961353302\n",
      "  batch 40 loss: 3.377959132194519\n",
      "  batch 50 loss: 3.167638874053955\n",
      "  batch 60 loss: 2.350830388069153\n",
      "  batch 70 loss: 3.424213969707489\n",
      "EPOCH 41:\n",
      "  batch 10 loss: 2.800844931602478\n",
      "  batch 20 loss: 4.287694692611694\n",
      "  batch 30 loss: 3.531902241706848\n",
      "  batch 40 loss: 2.3051437377929687\n",
      "  batch 50 loss: 3.517059123516083\n",
      "  batch 60 loss: 3.2835193991661074\n",
      "  batch 70 loss: 2.400858521461487\n",
      "EPOCH 42:\n",
      "  batch 10 loss: 2.736487662792206\n",
      "  batch 20 loss: 2.8327104687690734\n",
      "  batch 30 loss: 2.733058202266693\n",
      "  batch 40 loss: 4.011982309818268\n",
      "  batch 50 loss: 3.487699818611145\n",
      "  batch 60 loss: 2.4128979325294493\n",
      "  batch 70 loss: 2.362410569190979\n",
      "EPOCH 43:\n",
      "  batch 10 loss: 2.281028616428375\n",
      "  batch 20 loss: 3.5757741212844847\n",
      "  batch 30 loss: 3.117119860649109\n",
      "  batch 40 loss: 4.043199670314789\n",
      "  batch 50 loss: 3.0349887013435364\n",
      "  batch 60 loss: 3.383105194568634\n",
      "  batch 70 loss: 2.181764233112335\n",
      "EPOCH 44:\n",
      "  batch 10 loss: 3.3674703359603884\n",
      "  batch 20 loss: 3.3341492056846618\n",
      "  batch 30 loss: 3.981766104698181\n",
      "  batch 40 loss: 3.241260313987732\n",
      "  batch 50 loss: 2.6462493538856506\n",
      "  batch 60 loss: 2.2785428047180174\n",
      "  batch 70 loss: 3.2506118416786194\n",
      "EPOCH 45:\n",
      "  batch 10 loss: 2.233748507499695\n",
      "  batch 20 loss: 3.419938659667969\n",
      "  batch 30 loss: 4.1629973411560055\n",
      "  batch 40 loss: 2.855861580371857\n",
      "  batch 50 loss: 3.529102158546448\n",
      "  batch 60 loss: 2.475210499763489\n",
      "  batch 70 loss: 3.000413966178894\n",
      "EPOCH 46:\n",
      "  batch 10 loss: 3.676025938987732\n",
      "  batch 20 loss: 3.539308047294617\n",
      "  batch 30 loss: 2.7958555936813356\n",
      "  batch 40 loss: 3.204802989959717\n",
      "  batch 50 loss: 3.0122739553451536\n",
      "  batch 60 loss: 2.9838236212730407\n",
      "  batch 70 loss: 3.358739268779755\n",
      "EPOCH 47:\n",
      "  batch 10 loss: 3.6552032947540285\n",
      "  batch 20 loss: 2.737730348110199\n",
      "  batch 30 loss: 4.115673983097077\n",
      "  batch 40 loss: 2.8585031151771547\n",
      "  batch 50 loss: 3.2892666459083557\n",
      "  batch 60 loss: 2.5794183015823364\n",
      "  batch 70 loss: 2.828999030590057\n",
      "EPOCH 48:\n",
      "  batch 10 loss: 3.280874502658844\n",
      "  batch 20 loss: 2.5934337615966796\n",
      "  batch 30 loss: 3.679718387126923\n",
      "  batch 40 loss: 3.140250897407532\n",
      "  batch 50 loss: 3.011992883682251\n",
      "  batch 60 loss: 2.6772558212280275\n",
      "  batch 70 loss: 3.779381585121155\n",
      "EPOCH 49:\n",
      "  batch 10 loss: 2.189990758895874\n",
      "  batch 20 loss: 3.401228439807892\n",
      "  batch 30 loss: 3.497554075717926\n",
      "  batch 40 loss: 3.057728576660156\n",
      "  batch 50 loss: 3.8939002990722655\n",
      "  batch 60 loss: 2.8787450909614565\n",
      "  batch 70 loss: 2.5460869908332824\n",
      "EPOCH 50:\n",
      "  batch 10 loss: 2.972788166999817\n",
      "  batch 20 loss: 2.9913082480430604\n",
      "  batch 30 loss: 3.592926585674286\n",
      "  batch 40 loss: 2.9870561838150023\n",
      "  batch 50 loss: 3.0712362408638\n",
      "  batch 60 loss: 3.591473066806793\n",
      "  batch 70 loss: 2.0466549396514893\n",
      "EPOCH 51:\n",
      "  batch 10 loss: 3.479234766960144\n",
      "  batch 20 loss: 2.8163873672485353\n",
      "  batch 30 loss: 2.986209285259247\n",
      "  batch 40 loss: 2.9999094367027284\n",
      "  batch 50 loss: 2.799009907245636\n",
      "  batch 60 loss: 3.1789329051971436\n",
      "  batch 70 loss: 3.0516894817352296\n",
      "EPOCH 52:\n",
      "  batch 10 loss: 2.801610791683197\n",
      "  batch 20 loss: 3.4688947796821594\n",
      "  batch 30 loss: 2.6457507014274597\n",
      "  batch 40 loss: 2.208741068840027\n",
      "  batch 50 loss: 3.450396144390106\n",
      "  batch 60 loss: 3.0015065908432006\n",
      "  batch 70 loss: 4.569875359535217\n",
      "EPOCH 53:\n",
      "  batch 10 loss: 2.78475843667984\n",
      "  batch 20 loss: 4.610346412658691\n",
      "  batch 30 loss: 2.459741246700287\n",
      "  batch 40 loss: 2.5739023447036744\n",
      "  batch 50 loss: 2.939268434047699\n",
      "  batch 60 loss: 3.1913116812705993\n",
      "  batch 70 loss: 2.561552095413208\n",
      "EPOCH 54:\n",
      "  batch 10 loss: 3.0646178007125853\n",
      "  batch 20 loss: 3.001765263080597\n",
      "  batch 30 loss: 3.658202922344208\n",
      "  batch 40 loss: 2.791807842254639\n",
      "  batch 50 loss: 3.3082162857055666\n",
      "  batch 60 loss: 2.704957163333893\n",
      "  batch 70 loss: 2.645584535598755\n",
      "EPOCH 55:\n",
      "  batch 10 loss: 4.091148638725281\n",
      "  batch 20 loss: 3.4721584796905516\n",
      "  batch 30 loss: 2.5450973510742188\n",
      "  batch 40 loss: 3.4361779928207397\n",
      "  batch 50 loss: 2.6937435388565065\n",
      "  batch 60 loss: 3.0020135164260866\n",
      "  batch 70 loss: 3.0310646057128907\n",
      "EPOCH 56:\n",
      "  batch 10 loss: 3.150204086303711\n",
      "  batch 20 loss: 3.9989531457424166\n",
      "  batch 30 loss: 3.4819544434547423\n",
      "  batch 40 loss: 2.5923765182495115\n",
      "  batch 50 loss: 2.660498654842377\n",
      "  batch 60 loss: 3.109975981712341\n",
      "  batch 70 loss: 2.545522117614746\n",
      "EPOCH 57:\n",
      "  batch 10 loss: 3.7201791405677795\n",
      "  batch 20 loss: 2.54992755651474\n",
      "  batch 30 loss: 3.0802119374275208\n",
      "  batch 40 loss: 3.226349115371704\n",
      "  batch 50 loss: 2.995620942115784\n",
      "  batch 60 loss: 2.90008704662323\n",
      "  batch 70 loss: 3.4107625246047975\n",
      "EPOCH 58:\n",
      "  batch 10 loss: 3.729637622833252\n",
      "  batch 20 loss: 3.1893476963043215\n",
      "  batch 30 loss: 3.0222973465919494\n",
      "  batch 40 loss: 2.890162491798401\n",
      "  batch 50 loss: 2.7988026976585387\n",
      "  batch 60 loss: 3.4639071822166443\n",
      "  batch 70 loss: 2.797786593437195\n",
      "EPOCH 59:\n",
      "  batch 10 loss: 2.5808323860168456\n",
      "  batch 20 loss: 2.576691150665283\n",
      "  batch 30 loss: 2.760394334793091\n",
      "  batch 40 loss: 3.7409592866897583\n",
      "  batch 50 loss: 3.4463462710380552\n",
      "  batch 60 loss: 3.3277095794677733\n",
      "  batch 70 loss: 2.5360032320022583\n",
      "EPOCH 60:\n",
      "  batch 10 loss: 2.979800271987915\n",
      "  batch 20 loss: 2.716903865337372\n",
      "  batch 30 loss: 3.6615317821502686\n",
      "  batch 40 loss: 3.1861759424209595\n",
      "  batch 50 loss: 2.844614291191101\n",
      "  batch 60 loss: 4.080355477333069\n",
      "  batch 70 loss: 2.020972156524658\n",
      "EPOCH 61:\n",
      "  batch 10 loss: 3.018069791793823\n",
      "  batch 20 loss: 2.8748411893844605\n",
      "  batch 30 loss: 3.2827142000198366\n",
      "  batch 40 loss: 3.1708287477493284\n",
      "  batch 50 loss: 2.8470686793327333\n",
      "  batch 60 loss: 2.5815956354141236\n",
      "  batch 70 loss: 3.687987172603607\n",
      "EPOCH 62:\n",
      "  batch 10 loss: 5.095843267440796\n",
      "  batch 20 loss: 2.435481321811676\n",
      "  batch 30 loss: 3.5373746752738953\n",
      "  batch 40 loss: 3.4061389207839965\n",
      "  batch 50 loss: 2.3052119255065917\n",
      "  batch 60 loss: 2.101376402378082\n",
      "  batch 70 loss: 3.2823788642883303\n",
      "EPOCH 63:\n",
      "  batch 10 loss: 2.761879098415375\n",
      "  batch 20 loss: 3.353253650665283\n",
      "  batch 30 loss: 3.302309238910675\n",
      "  batch 40 loss: 2.697573411464691\n",
      "  batch 50 loss: 3.1244711756706236\n",
      "  batch 60 loss: 2.5758631825447083\n",
      "  batch 70 loss: 2.778480440378189\n",
      "EPOCH 64:\n",
      "  batch 10 loss: 3.2261150598526003\n",
      "  batch 20 loss: 3.0749805569648743\n",
      "  batch 30 loss: 2.3057544589042664\n",
      "  batch 40 loss: 3.2387349605560303\n",
      "  batch 50 loss: 3.932170534133911\n",
      "  batch 60 loss: 2.4829491496086122\n",
      "  batch 70 loss: 3.17544971704483\n",
      "EPOCH 65:\n",
      "  batch 10 loss: 3.209090459346771\n",
      "  batch 20 loss: 2.0148000478744508\n",
      "  batch 30 loss: 3.230886924266815\n",
      "  batch 40 loss: 3.370829403400421\n",
      "  batch 50 loss: 2.9495634913444517\n",
      "  batch 60 loss: 2.7604645133018493\n",
      "  batch 70 loss: 3.146693730354309\n",
      "EPOCH 66:\n",
      "  batch 10 loss: 3.8273146390914916\n",
      "  batch 20 loss: 2.795996332168579\n",
      "  batch 30 loss: 3.1457542359828947\n",
      "  batch 40 loss: 3.1610854506492614\n",
      "  batch 50 loss: 2.411525863409042\n",
      "  batch 60 loss: 2.712624359130859\n",
      "  batch 70 loss: 3.092511308193207\n",
      "EPOCH 67:\n",
      "  batch 10 loss: 2.373822832107544\n",
      "  batch 20 loss: 3.505493140220642\n",
      "  batch 30 loss: 3.478166425228119\n",
      "  batch 40 loss: 2.7621095180511475\n",
      "  batch 50 loss: 2.7826298773288727\n",
      "  batch 60 loss: 2.9419408082962035\n",
      "  batch 70 loss: 3.4307267665863037\n",
      "EPOCH 68:\n",
      "  batch 10 loss: 3.175362539291382\n",
      "  batch 20 loss: 2.584455645084381\n",
      "  batch 30 loss: 2.60292809009552\n",
      "  batch 40 loss: 3.4754473328590394\n",
      "  batch 50 loss: 3.5061636567115784\n",
      "  batch 60 loss: 3.4340731739997863\n",
      "  batch 70 loss: 2.38674396276474\n",
      "EPOCH 69:\n",
      "  batch 10 loss: 2.5484118103981017\n",
      "  batch 20 loss: 2.9604751229286195\n",
      "  batch 30 loss: 2.651024854183197\n",
      "  batch 40 loss: 3.1316551208496093\n",
      "  batch 50 loss: 2.7583632349967955\n",
      "  batch 60 loss: 3.669059956073761\n",
      "  batch 70 loss: 3.0204869627952577\n",
      "EPOCH 70:\n",
      "  batch 10 loss: 3.028689742088318\n",
      "  batch 20 loss: 3.558760392665863\n",
      "  batch 30 loss: 3.0678719520568847\n",
      "  batch 40 loss: 2.3955277800559998\n",
      "  batch 50 loss: 2.3292763948440554\n",
      "  batch 60 loss: 2.9790379405021667\n",
      "  batch 70 loss: 2.737064552307129\n",
      "EPOCH 71:\n",
      "  batch 10 loss: 2.539437460899353\n",
      "  batch 20 loss: 3.3680081486701967\n",
      "  batch 30 loss: 3.8680015444755553\n",
      "  batch 40 loss: 2.2012510776519774\n",
      "  batch 50 loss: 2.5787744879722596\n",
      "  batch 60 loss: 3.2975515604019163\n",
      "  batch 70 loss: 2.636762487888336\n",
      "EPOCH 72:\n",
      "  batch 10 loss: 3.438317906856537\n",
      "  batch 20 loss: 2.719567608833313\n",
      "  batch 30 loss: 4.089759445190429\n",
      "  batch 40 loss: 2.9673489689826966\n",
      "  batch 50 loss: 2.4610439777374267\n",
      "  batch 60 loss: 2.8215072393417358\n",
      "  batch 70 loss: 2.475423276424408\n",
      "EPOCH 73:\n",
      "  batch 10 loss: 3.0906909584999083\n",
      "  batch 20 loss: 2.7078839898109437\n",
      "  batch 30 loss: 2.542092525959015\n",
      "  batch 40 loss: 3.342925763130188\n",
      "  batch 50 loss: 3.676170027256012\n",
      "  batch 60 loss: 2.899251341819763\n",
      "  batch 70 loss: 2.4609692096710205\n",
      "EPOCH 74:\n",
      "  batch 10 loss: 3.0626986980438233\n",
      "  batch 20 loss: 2.755972123146057\n",
      "  batch 30 loss: 2.5979333996772764\n",
      "  batch 40 loss: 2.661227250099182\n",
      "  batch 50 loss: 2.4734105825424195\n",
      "  batch 60 loss: 2.853256893157959\n",
      "  batch 70 loss: 4.059502685070038\n",
      "EPOCH 75:\n",
      "  batch 10 loss: 2.3291057109832765\n",
      "  batch 20 loss: 3.3249316096305845\n",
      "  batch 30 loss: 2.859723520278931\n",
      "  batch 40 loss: 3.156297028064728\n",
      "  batch 50 loss: 3.8199767589569094\n",
      "  batch 60 loss: 2.755396282672882\n",
      "  batch 70 loss: 2.331192362308502\n",
      "EPOCH 76:\n",
      "  batch 10 loss: 2.3977440357208253\n",
      "  batch 20 loss: 2.7508550882339478\n",
      "  batch 30 loss: 2.667583179473877\n",
      "  batch 40 loss: 3.102146768569946\n",
      "  batch 50 loss: 2.878622627258301\n",
      "  batch 60 loss: 3.8047303199768066\n",
      "  batch 70 loss: 2.986077630519867\n",
      "EPOCH 77:\n",
      "  batch 10 loss: 2.9583753108978272\n",
      "  batch 20 loss: 2.9089126110076906\n",
      "  batch 30 loss: 2.414933753013611\n",
      "  batch 40 loss: 2.6972315549850463\n",
      "  batch 50 loss: 4.062365520000458\n",
      "  batch 60 loss: 3.11343115568161\n",
      "  batch 70 loss: 2.8801464080810546\n",
      "EPOCH 78:\n",
      "  batch 10 loss: 3.975268268585205\n",
      "  batch 20 loss: 3.568333077430725\n",
      "  batch 30 loss: 2.669382095336914\n",
      "  batch 40 loss: 2.3097711563110352\n",
      "  batch 50 loss: 2.492865252494812\n",
      "  batch 60 loss: 3.1813008189201355\n",
      "  batch 70 loss: 3.0477932572364805\n",
      "EPOCH 79:\n",
      "  batch 10 loss: 3.1064310312271117\n",
      "  batch 20 loss: 3.4023582458496096\n",
      "  batch 30 loss: 2.724898076057434\n",
      "  batch 40 loss: 3.1357760071754455\n",
      "  batch 50 loss: 3.523807096481323\n",
      "  batch 60 loss: 3.6957363963127134\n",
      "  batch 70 loss: 1.8589221358299255\n",
      "EPOCH 80:\n",
      "  batch 10 loss: 2.733837878704071\n",
      "  batch 20 loss: 3.243918812274933\n",
      "  batch 30 loss: 3.3643485426902773\n",
      "  batch 40 loss: 3.582483470439911\n",
      "  batch 50 loss: 3.364728260040283\n",
      "  batch 60 loss: 2.3687234044075014\n",
      "  batch 70 loss: 2.602976930141449\n",
      "EPOCH 81:\n",
      "  batch 10 loss: 3.4313178181648256\n",
      "  batch 20 loss: 2.3897700905799866\n",
      "  batch 30 loss: 3.120301878452301\n",
      "  batch 40 loss: 2.119423568248749\n",
      "  batch 50 loss: 4.354989302158356\n",
      "  batch 60 loss: 2.1616417169570923\n",
      "  batch 70 loss: 2.977918577194214\n",
      "EPOCH 82:\n",
      "  batch 10 loss: 3.1009459257125855\n",
      "  batch 20 loss: 3.0317084431648254\n",
      "  batch 30 loss: 2.935439336299896\n",
      "  batch 40 loss: 2.778036594390869\n",
      "  batch 50 loss: 3.3942287445068358\n",
      "  batch 60 loss: 2.7905316591262816\n",
      "  batch 70 loss: 3.2188652396202087\n",
      "EPOCH 83:\n",
      "  batch 10 loss: 2.7826482176780702\n",
      "  batch 20 loss: 3.22578387260437\n",
      "  batch 30 loss: 2.8500256538391113\n",
      "  batch 40 loss: 2.431530010700226\n",
      "  batch 50 loss: 4.863410305976868\n",
      "  batch 60 loss: 2.5061023950576784\n",
      "  batch 70 loss: 2.931558781862259\n",
      "EPOCH 84:\n",
      "  batch 10 loss: 3.1446038126945495\n",
      "  batch 20 loss: 3.048566687107086\n",
      "  batch 30 loss: 2.4589200615882874\n",
      "  batch 40 loss: 2.901520109176636\n",
      "  batch 50 loss: 2.0710379481315613\n",
      "  batch 60 loss: 2.9210219502449037\n",
      "  batch 70 loss: 3.723869061470032\n",
      "EPOCH 85:\n",
      "  batch 10 loss: 3.4569392919540407\n",
      "  batch 20 loss: 3.4682573556900023\n",
      "  batch 30 loss: 2.3212152242660524\n",
      "  batch 40 loss: 4.143623971939087\n",
      "  batch 50 loss: 2.7557483315467834\n",
      "  batch 60 loss: 3.153192639350891\n",
      "  batch 70 loss: 3.0815550327301025\n",
      "EPOCH 86:\n",
      "  batch 10 loss: 3.7859519958496093\n",
      "  batch 20 loss: 2.781500995159149\n",
      "  batch 30 loss: 2.8012856602668763\n",
      "  batch 40 loss: 2.5749969840049745\n",
      "  batch 50 loss: 3.509325850009918\n",
      "  batch 60 loss: 2.1706058979034424\n",
      "  batch 70 loss: 2.6350438714027407\n",
      "EPOCH 87:\n",
      "  batch 10 loss: 3.5555737733840944\n",
      "  batch 20 loss: 3.5833607792854307\n",
      "  batch 30 loss: 3.0789236068725585\n",
      "  batch 40 loss: 3.058281397819519\n",
      "  batch 50 loss: 2.647164857387543\n",
      "  batch 60 loss: 2.381705641746521\n",
      "  batch 70 loss: 2.4269428968429567\n",
      "EPOCH 88:\n",
      "  batch 10 loss: 2.957569718360901\n",
      "  batch 20 loss: 3.037995958328247\n",
      "  batch 30 loss: 2.2969515681266786\n",
      "  batch 40 loss: 3.027596044540405\n",
      "  batch 50 loss: 3.189050352573395\n",
      "  batch 60 loss: 3.231908643245697\n",
      "  batch 70 loss: 2.242945981025696\n",
      "EPOCH 89:\n",
      "  batch 10 loss: 3.4206061601638793\n",
      "  batch 20 loss: 2.3018789529800414\n",
      "  batch 30 loss: 3.4618149280548094\n",
      "  batch 40 loss: 3.673476958274841\n",
      "  batch 50 loss: 2.748191249370575\n",
      "  batch 60 loss: 2.49501690864563\n",
      "  batch 70 loss: 2.380599093437195\n",
      "EPOCH 90:\n",
      "  batch 10 loss: 2.810535264015198\n",
      "  batch 20 loss: 2.7112332463264464\n",
      "  batch 30 loss: 3.7150424480438233\n",
      "  batch 40 loss: 2.4974370121955873\n",
      "  batch 50 loss: 3.2406763553619387\n",
      "  batch 60 loss: 2.643199396133423\n",
      "  batch 70 loss: 2.9713552594184875\n",
      "EPOCH 91:\n",
      "  batch 10 loss: 2.7077537417411803\n",
      "  batch 20 loss: 2.5357351064682008\n",
      "  batch 30 loss: 3.2323818445205688\n",
      "  batch 40 loss: 2.983128213882446\n",
      "  batch 50 loss: 4.325204634666443\n",
      "  batch 60 loss: 2.7253633379936217\n",
      "  batch 70 loss: 2.892662823200226\n",
      "EPOCH 92:\n",
      "  batch 10 loss: 2.319453036785126\n",
      "  batch 20 loss: 2.9458497285842897\n",
      "  batch 30 loss: 2.432650864124298\n",
      "  batch 40 loss: 4.4866557121276855\n",
      "  batch 50 loss: 2.447000288963318\n",
      "  batch 60 loss: 3.133333909511566\n",
      "  batch 70 loss: 2.7915874481201173\n",
      "EPOCH 93:\n",
      "  batch 10 loss: 2.7103742718696595\n",
      "  batch 20 loss: 2.869019961357117\n",
      "  batch 30 loss: 2.5700024604797362\n",
      "  batch 40 loss: 2.38937771320343\n",
      "  batch 50 loss: 3.9754889011383057\n",
      "  batch 60 loss: 3.585121512413025\n",
      "  batch 70 loss: 2.2523965716361998\n",
      "EPOCH 94:\n",
      "  batch 10 loss: 2.7276153445243834\n",
      "  batch 20 loss: 3.0945971250534057\n",
      "  batch 30 loss: 3.449700617790222\n",
      "  batch 40 loss: 3.371326911449432\n",
      "  batch 50 loss: 2.8930672883987425\n",
      "  batch 60 loss: 2.407087194919586\n",
      "  batch 70 loss: 2.5139443278312683\n",
      "EPOCH 95:\n",
      "  batch 10 loss: 2.857908034324646\n",
      "  batch 20 loss: 2.3130655765533445\n",
      "  batch 30 loss: 2.6903008222579956\n",
      "  batch 40 loss: 2.899738383293152\n",
      "  batch 50 loss: 2.8008531808853148\n",
      "  batch 60 loss: 3.882972979545593\n",
      "  batch 70 loss: 2.935800564289093\n",
      "EPOCH 96:\n",
      "  batch 10 loss: 3.7186322093009947\n",
      "  batch 20 loss: 2.8005406141281126\n",
      "  batch 30 loss: 2.4654821276664736\n",
      "  batch 40 loss: 3.785631251335144\n",
      "  batch 50 loss: 2.7019150972366335\n",
      "  batch 60 loss: 2.8260470271110534\n",
      "  batch 70 loss: 2.2850249648094176\n",
      "EPOCH 97:\n",
      "  batch 10 loss: 3.0653690814971926\n",
      "  batch 20 loss: 2.2459454417228697\n",
      "  batch 30 loss: 3.084008824825287\n",
      "  batch 40 loss: 2.675683617591858\n",
      "  batch 50 loss: 3.579565703868866\n",
      "  batch 60 loss: 2.480236053466797\n",
      "  batch 70 loss: 2.474871551990509\n",
      "EPOCH 98:\n",
      "  batch 10 loss: 2.324869418144226\n",
      "  batch 20 loss: 3.0122800946235655\n",
      "  batch 30 loss: 2.73444287776947\n",
      "  batch 40 loss: 3.1263112902641295\n",
      "  batch 50 loss: 4.425981974601745\n",
      "  batch 60 loss: 1.9913370966911317\n",
      "  batch 70 loss: 3.324940228462219\n",
      "EPOCH 99:\n",
      "  batch 10 loss: 2.555987739562988\n",
      "  batch 20 loss: 2.567620098590851\n",
      "  batch 30 loss: 3.7121869921684265\n",
      "  batch 40 loss: 2.390066993236542\n",
      "  batch 50 loss: 3.565849232673645\n",
      "  batch 60 loss: 3.0562710523605348\n",
      "  batch 70 loss: 2.2240593075752257\n",
      "EPOCH 100:\n",
      "  batch 10 loss: 2.8199725985527038\n",
      "  batch 20 loss: 3.521935427188873\n",
      "  batch 30 loss: 3.296453905105591\n",
      "  batch 40 loss: 1.9423052430152894\n",
      "  batch 50 loss: 3.5908419609069826\n",
      "  batch 60 loss: 2.617246425151825\n",
      "  batch 70 loss: 2.58432537317276\n"
     ]
    }
   ],
   "source": [
    "# Code from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "EPOCHS = 100\n",
    "metrics_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch()\n",
    "\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    outputs = predict()\n",
    "    # Example of how to compute metrics\n",
    "    # metrics = compute_metrics(test_target.numpy(), outputs.squeeze().numpy())\n",
    "    # metrics_list.append(metrics)\n",
    "    \n",
    "    # if epoch % 10 == 9:\n",
    "    #     print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea83435-2e12-43a9-b2c1-3e2701cd119c",
   "metadata": {},
   "source": [
    "Lastly, we provide metrics that we calculated on the test set using **real** DDG targets:\n",
    "```\n",
    "'R2': 0.04984921216964722,\n",
    "'RMSE': 1.5236231\n",
    "'Pearson': 0.571105009522608\n",
    "'Spearman': 0.5379472889898176\n",
    "'StabSpearman': 0.47610780612378306\n",
    "'DetPr': 0.8669201520912547\n",
    "'nDCG': 0.921864101605235\n",
    "'MCC': 0.37286990274549875\n",
    "'AUC': 0.749912739965096\n",
    "'ACC': 0.6339581036383682\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04bf2e6-7f24-4acb-a34b-959b4d9120c2",
   "metadata": {},
   "source": [
    "As the result of the test task, we expect:\n",
    "- Reproducible code that trains the prediction model.\n",
    "- Predictions for the test dataset.\n",
    "- A detailed technical report on how the problem was approached. The technical report may include data analysis, experiment description, model architecture, etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
